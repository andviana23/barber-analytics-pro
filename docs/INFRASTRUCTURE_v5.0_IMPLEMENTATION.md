# Barber Analytics Pro - Implementa√ß√£o Infraestrutura v5.0

**Vers√£o:** 5.0
**Data:** 11 de novembro de 2025
**Autor:** Andrey Viana

Este documento complementa o `INFRASTRUCTURE_v4.0.md` (atualizado para v5.0) com detalhes pr√°ticos de implementa√ß√£o, mapeamento completo do sistema, fluxogramas de comunica√ß√£o e checklist execut√°vel.

## üìã √çndice

1. [Checklist de Implementa√ß√£o v5.0](#checklist-de-implementa√ß√£o-v50)
2. [Estrutura Completa do Projeto](#estrutura-completa-do-projeto)
3. [Mapeamento do Banco de Dados](#mapeamento-do-banco-de-dados)
4. [Fluxogramas de Comunica√ß√£o](#fluxogramas-de-comunica√ß√£o)
5. [Guia de Deploy](#guia-de-deploy)
6. [Troubleshooting](#troubleshooting)

---

## ‚úÖ Checklist de Implementa√ß√£o v5.0

### Fase 1: Prepara√ß√£o e An√°lise (‚úÖ CONCLU√çDO)

- [x] An√°lise completa do codebase (516 arquivos)
- [x] Mapeamento de tecnologias utilizadas
- [x] Identifica√ß√£o da arquitetura real (Vite+React vs Next.js)
- [x] Levantamento de todas as rotas API
- [x] Mapeamento de cron jobs configurados
- [x] An√°lise do schema do banco de dados
- [x] Identifica√ß√£o de integra√ß√µes externas

### Fase 2: Atualiza√ß√£o da Documenta√ß√£o (‚úÖ CONCLU√çDO)

- [x] Atualizar INFRASTRUCTURE.md para v5.0
- [x] Documentar arquitetura real (Vite+React + Serverless)
- [x] Criar diagrama de arquitetura atualizado
- [x] Mapear stack tecnol√≥gica completa
- [x] Adicionar se√ß√µes faltantes ao INFRASTRUCTURE.md principal:
  - [x] Estrutura completa do reposit√≥rio (ESTRUTURA_COMPLETA_REPOSITORIO.md - 46KB)
  - [x] Mapeamento de banco de dados (Se√ß√£o 3 deste documento - 23+ tabelas)
  - [x] Fluxo de dados detalhado (MAPEAMENTO_FLUXO_DADOS.md - 95KB)
  - [x] Integra√ß√µes externas (Se√ß√£o 9 do INFRASTRUCTURE.md - OpenAI, Telegram, OFX/Excel)
- [x] Criar documento de implementa√ß√£o (este arquivo)
- [x] Criar guias de refer√™ncia r√°pida:
  - [x] QUICK_REFERENCE_GUIDE.md (9KB)
  - [x] ARQUITETURA_VISUAL.md (39KB)
  - [x] DOCUMENTACAO_INDEX.md (11KB)

### Fase 3: Verifica√ß√£o de Implementa√ß√µes v4.0 (üîÑ A FAZER)

#### 3.1 - Idempot√™ncia

- [ ] Verificar se `lib/idempotency.ts` existe
- [ ] Verificar se tabela `etl_runs` existe no banco
- [ ] Verificar se todos os cron jobs usam `ensureIdempotency()`
- [ ] Testar idempot√™ncia com execu√ß√£o duplicada
- [ ] Validar timeout handling (10 minutos)

**Comando de verifica√ß√£o:**
```bash
# Verificar arquivo
ls -la lib/idempotency.ts

# Verificar uso nos cron jobs
grep -r "ensureIdempotency" app/api/cron/

# Verificar tabela no banco
npx supabase db dump --schema public -t etl_runs
```

#### 3.2 - Circuit Breaker

- [ ] Verificar se `lib/circuitBreaker.ts` existe
- [ ] Verificar implementa√ß√£o da classe `CircuitBreaker`
- [ ] Verificar inst√¢ncias singleton (`openaiCircuitBreaker`, `telegramCircuitBreaker`)
- [ ] Verificar uso em `lib/ai/openai.ts`
- [ ] Verificar uso em `lib/telegram.ts`
- [ ] Testar circuit breaker abrindo ap√≥s 5 falhas
- [ ] Testar reset ap√≥s timeout

**Comando de verifica√ß√£o:**
```bash
ls -la lib/circuitBreaker.ts
grep -r "CircuitBreaker" lib/
grep -r "openaiCircuitBreaker\|telegramCircuitBreaker" lib/
```

#### 3.3 - Retry com Backoff Exponencial

- [ ] Verificar se `lib/retry.ts` existe
- [ ] Verificar implementa√ß√£o de `retryWithBackoff()`
- [ ] Verificar par√¢metros: maxAttempts, initialDelay, backoffMultiplier
- [ ] Verificar uso em chamadas OpenAI
- [ ] Verificar uso em chamadas Telegram
- [ ] Testar retry com falha tempor√°ria
- [ ] Validar delays exponenciais (1s, 2s, 4s...)

**Comando de verifica√ß√£o:**
```bash
ls -la lib/retry.ts
grep -r "retryWithBackoff" lib/
```

#### 3.4 - Cache Inteligente OpenAI

- [ ] Verificar se `lib/cache.ts` existe
- [ ] Verificar fun√ß√µes: `getCachedAnalysis()`, `setCachedAnalysis()`, `generateCacheKey()`
- [ ] Verificar se tabela `openai_cache` existe
- [ ] Verificar indexes na tabela de cache
- [ ] Verificar TTL padr√£o (24h)
- [ ] Verificar integra√ß√£o com `lib/ai/openai.ts`
- [ ] Testar cache hit/miss
- [ ] Validar economia de custos (tracking)
- [ ] Verificar cleanup de cache antigo (fun√ß√£o SQL)

**Comando de verifica√ß√£o:**
```bash
ls -la lib/cache.ts
npx supabase db dump --schema public -t openai_cache
grep -r "getCachedAnalysis\|setCachedAnalysis" lib/
```

**Migration SQL necess√°ria:**
```sql
-- Verificar se tabela existe
SELECT table_name
FROM information_schema.tables
WHERE table_schema = 'public'
AND table_name = 'openai_cache';

-- Se n√£o existir, criar:
CREATE TABLE IF NOT EXISTS openai_cache (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  cache_key VARCHAR(255) UNIQUE NOT NULL,
  response TEXT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_openai_cache_key ON openai_cache(cache_key);
CREATE INDEX IF NOT EXISTS idx_openai_cache_created_at ON openai_cache(created_at);

-- Fun√ß√£o de cleanup
CREATE OR REPLACE FUNCTION fn_cleanup_old_cache()
RETURNS void AS $$
BEGIN
  DELETE FROM openai_cache
  WHERE created_at < NOW() - INTERVAL '7 days';
END;
$$ LANGUAGE plpgsql;
```

#### 3.5 - Structured Logging

- [ ] Verificar se `lib/logger.ts` existe
- [ ] Verificar classe `Logger` com m√©todos: info, error, warn, debug
- [ ] Verificar formata√ß√£o JSON estruturada
- [ ] Verificar gera√ß√£o de `correlationId`
- [ ] Verificar uso em todos os cron jobs
- [ ] Verificar uso em rotas API cr√≠ticas
- [ ] Validar campos: timestamp, level, message, context
- [ ] Testar logging em produ√ß√£o (Vercel Logs)

**Comando de verifica√ß√£o:**
```bash
ls -la lib/logger.ts
grep -r "import.*logger" app/api/
grep -r "logger\.info\|logger\.error" app/api/cron/
```

#### 3.6 - Monitoramento de Custos OpenAI

- [ ] Verificar se `lib/monitoring.ts` existe
- [ ] Verificar fun√ß√µes: `trackOpenAICost()`, `getMonthlyOpenAICost()`, `checkCostThreshold()`
- [ ] Verificar se tabela `openai_cost_tracking` existe
- [ ] Verificar indexes na tabela
- [ ] Verificar integra√ß√£o com chamadas OpenAI
- [ ] Configurar `OPENAI_COST_ALERT_THRESHOLD` (env var)
- [ ] Testar alerta aos 80% do threshold
- [ ] Validar tracking de tokens e custos

**Comando de verifica√ß√£o:**
```bash
ls -la lib/monitoring.ts
npx supabase db dump --schema public -t openai_cost_tracking
grep -r "trackOpenAICost\|getMonthlyOpenAICost" lib/
```

**Migration SQL necess√°ria:**
```sql
CREATE TABLE IF NOT EXISTS openai_cost_tracking (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id),
  date DATE NOT NULL,
  tokens_used INTEGER NOT NULL,
  cost_usd DECIMAL(10, 4) NOT NULL,
  model VARCHAR(50) NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_cost_tracking_date ON openai_cost_tracking(date);
CREATE INDEX IF NOT EXISTS idx_cost_tracking_unit_date ON openai_cost_tracking(unit_id, date);
```

#### 3.7 - Processamento Paralelo

- [ ] Verificar se `lib/parallelProcessing.ts` existe (ou similar)
- [ ] Verificar fun√ß√£o `processInBatches()`
- [ ] Verificar batch size configur√°vel (padr√£o: 5)
- [ ] Verificar uso no ETL di√°rio
- [ ] Testar processamento de m√∫ltiplas unidades
- [ ] Validar que n√£o ultrapassa timeout de fun√ß√£o (10s)
- [ ] Medir melhoria de performance vs sequencial

**Comando de verifica√ß√£o:**
```bash
ls -la lib/parallelProcessing.ts lib/parallel*.ts
grep -r "processInBatches\|Promise\.all" app/api/cron/etl-diario/
```

#### 3.8 - Health Checks

- [ ] Verificar se `/app/api/health/route.ts` existe (b√°sico)
- [ ] Verificar se `/app/api/health/detailed/route.ts` existe
- [ ] Verificar se `/app/api/cron/health-check/route.ts` existe
- [ ] Verificar cron job configurado (`*/5 * * * *`)
- [ ] Verificar checks: Supabase, OpenAI, Telegram, last cron, storage
- [ ] Testar resposta de cada check
- [ ] Validar alertas Telegram quando unhealthy
- [ ] Configurar `HEALTH_CHECK_ENABLED=true`

**Comando de verifica√ß√£o:**
```bash
ls -la app/api/health/route.ts
ls -la app/api/health/detailed/route.ts
ls -la app/api/cron/health-check/route.ts
grep -A5 "health-check" vercel.json
```

### Fase 4: Cria√ß√£o de Componentes Faltantes (üîÑ A FAZER)

Para cada item marcado como faltante acima, criar o arquivo/tabela correspondente seguindo os exemplos documentados no `INFRASTRUCTURE_v4.0.md` se√ß√£o "Melhorias v4.0".

**Prioridade Alta:**
1. `lib/idempotency.ts` + tabela `etl_runs`
2. `lib/cache.ts` + tabela `openai_cache`
3. `lib/monitoring.ts` + tabela `openai_cost_tracking`
4. `lib/logger.ts`
5. `lib/circuitBreaker.ts`
6. `lib/retry.ts`

**Prioridade M√©dia:**
7. `lib/parallelProcessing.ts`
8. `/app/api/health/detailed/route.ts`
9. `/app/api/cron/health-check/route.ts`

**Prioridade Baixa:**
10. Dashboard de sa√∫de (`/src/pages/AdminHealthPage.jsx`)
11. M√©tricas customizadas (`lib/metrics.ts`)

### Fase 5: Configura√ß√£o de Ambiente (üîÑ A FAZER)

#### 5.1 - Vari√°veis de Ambiente

Verificar se todas as vari√°veis est√£o configuradas no Vercel:

**Obrigat√≥rias:**
- [ ] `VITE_SUPABASE_URL`
- [ ] `VITE_SUPABASE_ANON_KEY`
- [ ] `SUPABASE_SERVICE_ROLE_KEY`
- [ ] `OPENAI_API_KEY`
- [ ] `TELEGRAM_BOT_TOKEN`
- [ ] `TELEGRAM_CHAT_ID`
- [ ] `CRON_SECRET`

**v5.0 Novas:**
- [ ] `OPENAI_MODEL` (default: gpt-4o-mini)
- [ ] `OPENAI_MODEL_FALLBACK` (default: gpt-3.5-turbo)
- [ ] `OPENAI_COST_ALERT_THRESHOLD` (default: 80)
- [ ] `HEALTH_CHECK_ENABLED` (default: true)
- [ ] `REDIS_URL` (opcional, para cache externo)
- [ ] `ANALYTICS_BATCH_SIZE` (default: 5)
- [ ] `LOG_LEVEL` (default: info)
- [ ] `ENABLE_STRUCTURED_LOGGING` (default: true)

**Comando de verifica√ß√£o:**
```bash
vercel env ls
vercel env pull .env.local
```

#### 5.2 - Vercel Configuration

- [ ] Verificar `vercel.json` existe
- [ ] Verificar cron jobs configurados (7 jobs)
- [ ] Verificar headers CSP/HSTS/Security
- [ ] Verificar regions (`gru1` para Brasil)
- [ ] Verificar redirects e rewrites para SPA
- [ ] Verificar build settings (Vite)

**Comando de verifica√ß√£o:**
```bash
cat vercel.json
```

**Conte√∫do esperado do `vercel.json`:**
```json
{
  "buildCommand": "pnpm build",
  "devCommand": "pnpm dev",
  "framework": null,
  "installCommand": "pnpm install",
  "outputDirectory": "dist",
  "regions": ["gru1"],
  "crons": [
    {
      "path": "/api/cron/etl-diario",
      "schedule": "0 3 * * *"
    },
    {
      "path": "/api/cron/relatorio-semanal",
      "schedule": "0 6 * * 1"
    },
    {
      "path": "/api/cron/fechamento-mensal",
      "schedule": "0 7 1 * *"
    },
    {
      "path": "/api/cron/enviar-alertas",
      "schedule": "*/15 * * * *"
    },
    {
      "path": "/api/cron/health-check",
      "schedule": "*/5 * * * *"
    },
    {
      "path": "/api/cron/validate-balance",
      "schedule": "0 4 * * *"
    },
    {
      "path": "/api/cron/gerar-despesas-recorrentes",
      "schedule": "0 2 * * *"
    }
  ],
  "headers": [
    {
      "source": "/(.*)",
      "headers": [
        {
          "key": "X-Content-Type-Options",
          "value": "nosniff"
        },
        {
          "key": "X-Frame-Options",
          "value": "DENY"
        },
        {
          "key": "X-XSS-Protection",
          "value": "1; mode=block"
        },
        {
          "key": "Referrer-Policy",
          "value": "strict-origin-when-cross-origin"
        },
        {
          "key": "Permissions-Policy",
          "value": "camera=(), microphone=(), geolocation=()"
        },
        {
          "key": "Strict-Transport-Security",
          "value": "max-age=31536000; includeSubDomains"
        }
      ]
    }
  ],
  "rewrites": [
    {
      "source": "/((?!api).*)",
      "destination": "/index.html"
    }
  ]
}
```

#### 5.3 - Supabase Configuration

- [ ] Verificar conex√£o com Supabase
- [ ] Verificar RLS habilitado em todas as tabelas
- [ ] Verificar policies por unidade/tenant
- [ ] Verificar PgBouncer habilitado
- [ ] Verificar backups autom√°ticos configurados
- [ ] Verificar migrations aplicadas (38 migrations)

**Comando de verifica√ß√£o:**
```bash
npx supabase status
npx supabase db diff
npx supabase migration list
```

### Fase 6: Testes e Valida√ß√£o (üîÑ A FAZER)

#### 6.1 - Testes Unit√°rios

- [ ] Executar suite de testes: `pnpm test`
- [ ] Verificar cobertura de testes: `pnpm test:coverage`
- [ ] Validar threshold m√≠nimo (target: 80%)
- [ ] Corrigir testes quebrados

**Comando:**
```bash
pnpm test
pnpm test:coverage
```

#### 6.2 - Testes de Integra√ß√£o

- [ ] Testar fluxo completo de ETL
- [ ] Testar integra√ß√£o Supabase
- [ ] Testar integra√ß√£o OpenAI
- [ ] Testar integra√ß√£o Telegram
- [ ] Testar cache hit/miss
- [ ] Testar circuit breaker
- [ ] Testar retry logic

**Scripts de teste:**
```bash
pnpm tsx scripts/test-etl.sh
pnpm tsx scripts/test-openai.ts
pnpm tsx scripts/test-telegram.ts
```

#### 6.3 - Testes E2E

- [ ] Executar testes Playwright: `pnpm test:e2e`
- [ ] Testar fluxos cr√≠ticos de usu√°rio
- [ ] Testar responsividade mobile
- [ ] Testar acessibilidade (axe-core)

**Comando:**
```bash
pnpm test:e2e
pnpm test:e2e:ui
```

#### 6.4 - Testes de Cron Jobs

Para cada cron job, testar execu√ß√£o manual:

```bash
# ETL Di√°rio
curl -X GET https://seu-dominio.vercel.app/api/cron/etl-diario \
  -H "Authorization: Bearer $CRON_SECRET"

# Relat√≥rio Semanal
curl -X GET https://seu-dominio.vercel.app/api/cron/relatorio-semanal \
  -H "Authorization: Bearer $CRON_SECRET"

# Health Check
curl -X GET https://seu-dominio.vercel.app/api/cron/health-check \
  -H "Authorization: Bearer $CRON_SECRET"
```

- [ ] ETL Di√°rio executa sem erros
- [ ] Idempot√™ncia funciona (segunda execu√ß√£o √© skipped)
- [ ] Relat√≥rio semanal gera PDF/dados
- [ ] Fechamento mensal calcula DRE
- [ ] Envio de alertas notifica Telegram
- [ ] Valida√ß√£o de saldo detecta diverg√™ncias
- [ ] Despesas recorrentes s√£o criadas
- [ ] Health check valida todos os servi√ßos

#### 6.5 - Testes de Performance

- [ ] Medir tempo de build: `time pnpm build`
- [ ] Medir tamanho do bundle: `du -sh dist/`
- [ ] Medir tempo de ETL por unidade
- [ ] Validar que fun√ß√µes n√£o ultrapassam 10s
- [ ] Testar com carga (10+ unidades simult√¢neas)
- [ ] Validar uso de mem√≥ria < 1GB

**M√©tricas esperadas:**
- Build time: < 2 minutos
- Bundle size: < 2 MB (gzipped)
- ETL por unidade: < 5s
- Fun√ß√£o total: < 300s (com processamento paralelo)

### Fase 7: Deploy e Monitoramento (üîÑ A FAZER)

#### 7.1 - Deploy Preview

- [ ] Criar branch `feature/infra-v5`
- [ ] Push para GitHub
- [ ] Aguardar preview deploy Vercel
- [ ] Validar preview URL funciona
- [ ] Executar smoke tests no preview
- [ ] Validar cron jobs no preview (verificar logs)

**Comando:**
```bash
git checkout -b feature/infra-v5
git add .
git commit -m "feat: implement infrastructure v5.0"
git push origin feature/infra-v5
```

#### 7.2 - Deploy Production

- [ ] Criar Pull Request para `main`
- [ ] Code review aprovado
- [ ] CI/CD passa (lint, test, build)
- [ ] Merge para `main`
- [ ] Aguardar production deploy
- [ ] Validar production URL
- [ ] Executar smoke tests em produ√ß√£o
- [ ] Monitorar logs por 24h

#### 7.3 - Configura√ß√£o de Monitoramento

- [ ] Configurar alertas Vercel (erros, performance)
- [ ] Configurar alertas Supabase (quota, performance)
- [ ] Configurar alertas OpenAI (custos, quota)
- [ ] Configurar alertas Telegram (health checks)
- [ ] Configurar dashboard Vercel Analytics
- [ ] Configurar dashboard Supabase (queries lentas)

#### 7.4 - Valida√ß√£o P√≥s-Deploy

- [ ] Verificar primeiro ETL di√°rio executa √†s 03:00
- [ ] Verificar notifica√ß√£o Telegram recebida
- [ ] Verificar dados salvos no banco
- [ ] Verificar cache funcionando (economia de custos)
- [ ] Verificar health checks executando a cada 5min
- [ ] Verificar logs estruturados no Vercel
- [ ] Verificar m√©tricas de performance
- [ ] Validar custos OpenAI dentro do esperado

### Fase 8: Documenta√ß√£o Final (üîÑ A FAZER)

- [ ] Atualizar README.md com arquitetura v5.0
- [ ] Criar/atualizar diagramas de arquitetura
- [ ] Documentar todos os endpoints API
- [ ] Documentar schema do banco de dados
- [ ] Criar guia de troubleshooting
- [ ] Documentar processo de deploy
- [ ] Criar guia de desenvolvimento local
- [ ] Atualizar CHANGELOG.md

### Resumo de Status

| Fase | Status | Progresso |
|------|--------|-----------|
| 1. Prepara√ß√£o e An√°lise | ‚úÖ Completo | 100% |
| 2. Atualiza√ß√£o Documenta√ß√£o | üöß Em Andamento | 60% |
| 3. Verifica√ß√£o v4.0 | ‚è≥ Pendente | 0% |
| 4. Cria√ß√£o de Componentes | ‚è≥ Pendente | 0% |
| 5. Configura√ß√£o Ambiente | ‚è≥ Pendente | 0% |
| 6. Testes e Valida√ß√£o | ‚è≥ Pendente | 0% |
| 7. Deploy e Monitoramento | ‚è≥ Pendente | 0% |
| 8. Documenta√ß√£o Final | ‚è≥ Pendente | 0% |

**Progresso Geral:** ~20%

---

## üóÇÔ∏è Estrutura Completa do Projeto

### √Årvore de Diret√≥rios (Simplificada)

```
barber-analytics-pro/
‚îú‚îÄ‚îÄ .github/                      # GitHub Actions workflows
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci.yml               # Lint, test, build
‚îÇ       ‚îî‚îÄ‚îÄ deploy.yml           # Deploy automation
‚îÇ
‚îú‚îÄ‚îÄ .husky/                      # Git hooks
‚îÇ   ‚îú‚îÄ‚îÄ pre-commit               # Lint-staged
‚îÇ   ‚îî‚îÄ‚îÄ commit-msg               # Commitlint
‚îÇ
‚îú‚îÄ‚îÄ app/                         # Backend - Serverless API Routes
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îú‚îÄ‚îÄ alerts/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ query/           # GET /api/alerts/query
‚îÇ       ‚îú‚îÄ‚îÄ cron/                # Scheduled Tasks
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ etl-diario/      # Daily ETL (03:00)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ relatorio-semanal/ # Weekly report (Mon 06:00)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ fechamento-mensal/ # Monthly closing (1st 07:00)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ enviar-alertas/  # Send alerts (every 15min)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ health-check/    # Health monitoring (every 5min)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ validate-balance/ # Balance validation (04:00)
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ gerar-despesas-recorrentes/ # Recurring expenses (02:00)
‚îÇ       ‚îú‚îÄ‚îÄ forecasts/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ cashflow/        # GET/POST /api/forecasts/cashflow
‚îÇ       ‚îú‚îÄ‚îÄ kpis/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ health/          # GET /api/kpis/health
‚îÇ       ‚îú‚îÄ‚îÄ reports/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ weekly/          # GET /api/reports/weekly
‚îÇ       ‚îî‚îÄ‚îÄ telegram/
‚îÇ           ‚îî‚îÄ‚îÄ webhook/         # POST /api/telegram/webhook
‚îÇ
‚îú‚îÄ‚îÄ lib/                         # Backend Business Logic
‚îÇ   ‚îú‚îÄ‚îÄ ai/                      # AI/OpenAI Integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.ts           # OpenAI client + cache
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis.ts         # AI analysis logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anonymization.ts    # Data anonymization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.ts          # AI prompts
‚îÇ   ‚îú‚îÄ‚îÄ analytics/               # ETL & Analytics Engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ etl.ts              # Main ETL pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calculations.ts     # Financial calculations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anomalies.ts        # Anomaly detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cashflowForecast.ts # Cashflow predictions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validateBalance.ts  # Balance validation
‚îÇ   ‚îú‚îÄ‚îÄ repositories/            # Data Access Layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiMetricsRepository.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alertsRepository.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kpiTargetsRepository.ts
‚îÇ   ‚îú‚îÄ‚îÄ middleware/              # API Middleware
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cronAuth.ts         # Cron authentication
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rateLimit.ts        # Rate limiting
‚îÇ   ‚îú‚îÄ‚îÄ services/                # Backend Services
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recurringExpenseNotifications.ts
‚îÇ   ‚îú‚îÄ‚îÄ telegram/                # Telegram Integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commands.ts
‚îÇ   ‚îú‚îÄ‚îÄ supabaseAdmin.ts        # Supabase admin client
‚îÇ   ‚îú‚îÄ‚îÄ idempotency.ts          # ‚ö° v4.0 - Idempotency control
‚îÇ   ‚îú‚îÄ‚îÄ cache.ts                # ‚ö° v4.0 - Cache layer
‚îÇ   ‚îú‚îÄ‚îÄ circuitBreaker.ts       # ‚ö° v4.0 - Circuit breaker
‚îÇ   ‚îú‚îÄ‚îÄ retry.ts                # ‚ö° v4.0 - Retry logic
‚îÇ   ‚îú‚îÄ‚îÄ logger.ts               # ‚ö° v4.0 - Structured logging
‚îÇ   ‚îú‚îÄ‚îÄ monitoring.ts           # ‚ö° v4.0 - Cost monitoring
‚îÇ   ‚îî‚îÄ‚îÄ parallelProcessing.ts   # ‚ö° v4.0 - Parallel processing
‚îÇ
‚îú‚îÄ‚îÄ src/                         # Frontend - Vite + React Application
‚îÇ   ‚îú‚îÄ‚îÄ atoms/                   # Atomic Design - Level 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Button/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Input/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Badge/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (15+ atomic components)
‚îÇ   ‚îú‚îÄ‚îÄ molecules/               # Atomic Design - Level 2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FormField/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Card/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Alert/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (20+ molecular components)
‚îÇ   ‚îú‚îÄ‚îÄ organisms/               # Atomic Design - Level 3
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Header/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Sidebar/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DataTable/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CommissionFormModal/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (30+ organism components)
‚îÇ   ‚îú‚îÄ‚îÄ templates/               # Page Templates
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DashboardLayout/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AuthLayout/
‚îÇ   ‚îú‚îÄ‚îÄ pages/                   # Route Pages (19 pages)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DashboardPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RevenuesPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ExpensesPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BankReconciliationPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CashFlowPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DREPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ReportsPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GoalsPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AlertsPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CommissionsPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ListaDaVezPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OrdersPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ServicesPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProfessionalsPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ClientsPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SuppliersPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SettingsPage.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UserProfilePage.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LoginPage.jsx
‚îÇ   ‚îú‚îÄ‚îÄ services/                # Business Logic (43+ services)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ revenueService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ expenseService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reconciliationService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analyticsService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alertService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (38+ more services)
‚îÇ   ‚îú‚îÄ‚îÄ repositories/            # Data Access Layer (25+ repos)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ revenueRepository.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ expenseRepository.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unitsRepository.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ professionalsRepository.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (21+ more repositories)
‚îÇ   ‚îú‚îÄ‚îÄ hooks/                   # Custom React Hooks (30+ hooks)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useAuth.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useRevenues.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useExpenses.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useAlerts.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useKPIs.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (25+ more hooks)
‚îÇ   ‚îú‚îÄ‚îÄ context/                 # React Context Providers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AuthContext.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ThemeContext.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ NotificationContext.jsx
‚îÇ   ‚îú‚îÄ‚îÄ utils/                   # Utility Functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatters.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validators.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calculations.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exportCommissions.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ exportRevenues.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ testSupabaseConnection.js
‚îÇ   ‚îú‚îÄ‚îÄ dtos/                    # Data Transfer Objects
‚îÇ   ‚îú‚îÄ‚îÄ styles/                  # Global Styles
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ globals.css
‚îÇ   ‚îú‚îÄ‚îÄ main.jsx                 # ‚ö° Entry Point (Vite + React)
‚îÇ   ‚îî‚îÄ‚îÄ App.jsx                  # Root Component
‚îÇ
‚îú‚îÄ‚îÄ supabase/                    # Database
‚îÇ   ‚îú‚îÄ‚îÄ migrations/              # 38 SQL migrations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 20250101000000_initial_schema.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 20250102000000_add_reconciliation.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 20250308000000_add_etl_tracking.sql
‚îÇ   ‚îú‚îÄ‚îÄ functions/               # Edge Functions (if any)
‚îÇ   ‚îî‚îÄ‚îÄ config.toml             # Supabase config
‚îÇ
‚îú‚îÄ‚îÄ scripts/                     # Utility Scripts
‚îÇ   ‚îú‚îÄ‚îÄ run-etl.ts              # Manual ETL execution
‚îÇ   ‚îú‚îÄ‚îÄ test-openai.ts          # Test OpenAI integration
‚îÇ   ‚îú‚îÄ‚îÄ test-telegram.ts        # Test Telegram bot
‚îÇ   ‚îú‚îÄ‚îÄ get-telegram-chat-id.ts # Get Telegram chat ID
‚îÇ   ‚îú‚îÄ‚îÄ create-test-alert.ts    # Create test alert
‚îÇ   ‚îú‚îÄ‚îÄ test-etl.sh             # Shell script for ETL test
‚îÇ   ‚îî‚îÄ‚îÄ validate-rls.sql        # Validate RLS policies
‚îÇ
‚îú‚îÄ‚îÄ tests/                       # Tests
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ idempotency.test.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ margin.test.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ integration/
‚îÇ
‚îú‚îÄ‚îÄ e2e/                        # E2E Tests (Playwright)
‚îÇ   ‚îî‚îÄ‚îÄ ... playwright tests
‚îÇ
‚îú‚îÄ‚îÄ docs/                       # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ ETL_SEM_OPENAI.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ components/                 # Shared Component Library
‚îú‚îÄ‚îÄ public/                     # Static Assets
‚îÇ
‚îú‚îÄ‚îÄ .env.example                # Environment variables template
‚îú‚îÄ‚îÄ .eslintrc.cjs              # ESLint config
‚îú‚îÄ‚îÄ .prettierrc                # Prettier config
‚îú‚îÄ‚îÄ commitlint.config.js       # Commitlint config
‚îú‚îÄ‚îÄ playwright.config.js       # Playwright config
‚îú‚îÄ‚îÄ tailwind.config.js         # Tailwind config
‚îú‚îÄ‚îÄ tsconfig.json              # TypeScript config (backend)
‚îú‚îÄ‚îÄ tsconfig.cli.json          # TypeScript config (CLI scripts)
‚îú‚îÄ‚îÄ vite.config.js             # ‚ö° Vite config (build tool)
‚îú‚îÄ‚îÄ vitest.config.js           # Vitest config (testing)
‚îú‚îÄ‚îÄ vercel.json                # ‚ö° Vercel deployment config
‚îú‚îÄ‚îÄ package.json               # Dependencies & scripts
‚îú‚îÄ‚îÄ pnpm-lock.yaml             # Lock file
‚îî‚îÄ‚îÄ README.md                  # Main documentation
```

### Contadores

| Tipo | Quantidade |
|------|------------|
| Total de arquivos | 516 |
| P√°ginas React | 19 |
| Servi√ßos (business logic) | 43+ |
| Reposit√≥rios (data access) | 25+ |
| Custom Hooks | 30+ |
| Componentes Atoms | 15+ |
| Componentes Molecules | 20+ |
| Componentes Organisms | 30+ |
| API Routes | 12+ |
| Cron Jobs | 7 |
| Migrations SQL | 38 |
| Scripts utilit√°rios | 7 |
| Tests | V√°rios (unit + integration + e2e) |

---

## üíæ Mapeamento do Banco de Dados

### Schema Overview

**Provider:** Supabase (PostgreSQL 15)
**Schema:** `public`
**Total de tabelas:** 23+ core tables
**Migrations:** 38 arquivos SQL versionados
**RLS:** Habilitado em todas as tabelas sens√≠veis

### Tabelas Core

#### 1. **units** - Unidades de Neg√≥cio

Armazena as unidades/filiais da barbearia.

```sql
CREATE TABLE units (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL,
  address TEXT,
  phone VARCHAR(20),
  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_units_is_active ON units(is_active);

-- RLS Policy
ALTER TABLE units ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their unit"
  ON units FOR SELECT
  USING (auth.jwt() ->> 'unit_id' = id::text);
```

#### 2. **professionals** - Profissionais

Barbeiros e profissionais da unidade.

```sql
CREATE TABLE professionals (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  name VARCHAR(255) NOT NULL,
  email VARCHAR(255),
  phone VARCHAR(20),
  role VARCHAR(50), -- 'Barbeiro', 'Gerente', etc.
  commission_rate DECIMAL(5, 2) DEFAULT 0, -- Percentual de comiss√£o
  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_professionals_unit_id ON professionals(unit_id);
CREATE INDEX idx_professionals_is_active ON professionals(is_active);

-- RLS Policy
ALTER TABLE professionals ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view professionals from their unit"
  ON professionals FOR SELECT
  USING (unit_id::text = auth.jwt() ->> 'unit_id');
```

#### 3. **bank_accounts** - Contas Banc√°rias

```sql
CREATE TABLE bank_accounts (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  name VARCHAR(255) NOT NULL,
  bank_name VARCHAR(100),
  account_number VARCHAR(50),
  account_type VARCHAR(50), -- 'Corrente', 'Poupan√ßa'
  initial_balance DECIMAL(15, 2) DEFAULT 0,
  current_balance DECIMAL(15, 2) DEFAULT 0,
  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_bank_accounts_unit_id ON bank_accounts(unit_id);
CREATE INDEX idx_bank_accounts_is_active ON bank_accounts(is_active);
```

#### 4. **payment_methods** - M√©todos de Pagamento

```sql
CREATE TABLE payment_methods (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  name VARCHAR(100) NOT NULL,
  type VARCHAR(50) NOT NULL, -- 'PIX', 'D√©bito', 'Cr√©dito', 'Dinheiro', 'Transfer√™ncia'
  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_payment_methods_unit_id ON payment_methods(unit_id);
CREATE INDEX idx_payment_methods_type ON payment_methods(type);
```

#### 5. **parties** - Clientes e Fornecedores

```sql
CREATE TABLE parties (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  nome VARCHAR(255) NOT NULL,
  tipo VARCHAR(20) NOT NULL, -- 'Cliente' ou 'Fornecedor'
  cpf_cnpj VARCHAR(18),
  email VARCHAR(255),
  telefone VARCHAR(20),
  endereco TEXT,
  observacoes TEXT,
  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_parties_unit_id ON parties(unit_id);
CREATE INDEX idx_parties_tipo ON parties(tipo);
CREATE INDEX idx_parties_cpf_cnpj ON parties(cpf_cnpj);
```

#### 6. **categories** - Categorias de Receitas/Despesas

Sistema hier√°rquico de categorias.

```sql
CREATE TABLE categories (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  parent_id UUID REFERENCES categories(id) ON DELETE SET NULL,
  name VARCHAR(255) NOT NULL,
  category_type VARCHAR(20) NOT NULL, -- 'Receita' ou 'Despesa'
  description TEXT,
  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_categories_unit_id ON categories(unit_id);
CREATE INDEX idx_categories_parent_id ON categories(parent_id);
CREATE INDEX idx_categories_type ON categories(category_type);
```

#### 7. **revenues** - Receitas

```sql
CREATE TABLE revenues (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  category_id UUID REFERENCES categories(id),
  payment_method_id UUID REFERENCES payment_methods(id),
  bank_account_id UUID REFERENCES bank_accounts(id),
  client_id UUID REFERENCES parties(id),
  professional_id UUID REFERENCES professionals(id),

  descricao VARCHAR(500) NOT NULL,
  valor DECIMAL(15, 2) NOT NULL,
  data DATE NOT NULL,
  data_competencia DATE, -- Compet√™ncia cont√°bil

  -- Campos para concilia√ß√£o
  source_hash VARCHAR(64), -- Hash √∫nico para detectar duplicatas (OFX import)
  reconciled BOOLEAN DEFAULT false,
  reconciliation_id UUID,

  observacoes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_revenues_unit_id ON revenues(unit_id);
CREATE INDEX idx_revenues_data ON revenues(data);
CREATE INDEX idx_revenues_data_competencia ON revenues(data_competencia);
CREATE INDEX idx_revenues_category_id ON revenues(category_id);
CREATE INDEX idx_revenues_source_hash ON revenues(source_hash);
CREATE INDEX idx_revenues_reconciled ON revenues(reconciled);
```

#### 8. **expenses** - Despesas

```sql
CREATE TABLE expenses (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  category_id UUID REFERENCES categories(id),
  payment_method_id UUID REFERENCES payment_methods(id),
  bank_account_id UUID REFERENCES bank_accounts(id),
  supplier_id UUID REFERENCES parties(id),

  descricao VARCHAR(500) NOT NULL,
  valor DECIMAL(15, 2) NOT NULL,
  data DATE NOT NULL,
  data_competencia DATE,

  -- Campos para despesas recorrentes
  is_recurring BOOLEAN DEFAULT false,
  recurrence_frequency VARCHAR(20), -- 'Mensal', 'Trimestral', 'Anual'
  recurrence_day INTEGER, -- Dia do m√™s (1-31)

  -- Campos para concilia√ß√£o
  source_hash VARCHAR(64),
  reconciled BOOLEAN DEFAULT false,
  reconciliation_id UUID,

  observacoes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_expenses_unit_id ON expenses(unit_id);
CREATE INDEX idx_expenses_data ON expenses(data);
CREATE INDEX idx_expenses_data_competencia ON expenses(data_competencia);
CREATE INDEX idx_expenses_category_id ON expenses(category_id);
CREATE INDEX idx_expenses_is_recurring ON expenses(is_recurring);
```

#### 9. **bank_statements** - Extratos Banc√°rios Importados

```sql
CREATE TABLE bank_statements (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  bank_account_id UUID REFERENCES bank_accounts(id) ON DELETE CASCADE,

  transaction_date DATE NOT NULL,
  description TEXT NOT NULL,
  amount DECIMAL(15, 2) NOT NULL,
  transaction_type VARCHAR(20), -- 'Cr√©dito' ou 'D√©bito'
  balance_after DECIMAL(15, 2),

  -- Metadados do import
  import_date TIMESTAMPTZ DEFAULT NOW(),
  import_source VARCHAR(50), -- 'OFX', 'CSV', 'Excel'
  source_hash VARCHAR(64) UNIQUE, -- Para evitar duplicatas

  -- Status de reconcilia√ß√£o
  reconciled BOOLEAN DEFAULT false,
  reconciliation_id UUID,

  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_bank_statements_account_id ON bank_statements(bank_account_id);
CREATE INDEX idx_bank_statements_date ON bank_statements(transaction_date);
CREATE INDEX idx_bank_statements_reconciled ON bank_statements(reconciled);
CREATE INDEX idx_bank_statements_source_hash ON bank_statements(source_hash);
```

#### 10. **reconciliations** - Concilia√ß√µes Banc√°rias

```sql
CREATE TABLE reconciliations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  bank_statement_id UUID REFERENCES bank_statements(id),
  transaction_id UUID, -- Pode ser revenue_id ou expense_id
  transaction_type VARCHAR(20), -- 'Revenue' ou 'Expense'

  status VARCHAR(20) NOT NULL, -- 'Pending', 'Matched', 'Confirmed', 'Rejected'
  confidence_score DECIMAL(5, 2), -- 0-100 (para matching autom√°tico)
  match_method VARCHAR(50), -- 'Exact', 'Fuzzy', 'Manual'

  reconciled_by UUID, -- user_id
  reconciled_at TIMESTAMPTZ,

  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_reconciliations_statement_id ON reconciliations(bank_statement_id);
CREATE INDEX idx_reconciliations_transaction_id ON reconciliations(transaction_id);
CREATE INDEX idx_reconciliations_status ON reconciliations(status);
```

#### 11. **services** - Servi√ßos Oferecidos

```sql
CREATE TABLE services (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  name VARCHAR(255) NOT NULL,
  description TEXT,
  price DECIMAL(10, 2) NOT NULL,
  duration_minutes INTEGER, -- Dura√ß√£o em minutos
  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_services_unit_id ON services(unit_id);
CREATE INDEX idx_services_is_active ON services(is_active);
```

#### 12. **orders** - Pedidos/Atendimentos

```sql
CREATE TABLE orders (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  professional_id UUID REFERENCES professionals(id),
  client_id UUID REFERENCES parties(id),

  order_number VARCHAR(50) UNIQUE,
  status VARCHAR(20) NOT NULL, -- 'Pending', 'In Progress', 'Completed', 'Cancelled'
  total_amount DECIMAL(10, 2) NOT NULL,
  discount_amount DECIMAL(10, 2) DEFAULT 0,
  final_amount DECIMAL(10, 2) NOT NULL,

  payment_method_id UUID REFERENCES payment_methods(id),
  payment_status VARCHAR(20), -- 'Pending', 'Paid', 'Refunded'

  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  completed_at TIMESTAMPTZ
);

CREATE INDEX idx_orders_unit_id ON orders(unit_id);
CREATE INDEX idx_orders_professional_id ON orders(professional_id);
CREATE INDEX idx_orders_status ON orders(status);
CREATE INDEX idx_orders_created_at ON orders(created_at);
```

#### 13. **order_items** - Itens do Pedido

```sql
CREATE TABLE order_items (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  order_id UUID REFERENCES orders(id) ON DELETE CASCADE,
  service_id UUID REFERENCES services(id),

  quantity INTEGER DEFAULT 1,
  unit_price DECIMAL(10, 2) NOT NULL,
  total_price DECIMAL(10, 2) NOT NULL,

  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_order_items_order_id ON order_items(order_id);
CREATE INDEX idx_order_items_service_id ON order_items(service_id);
```

#### 14. **commissions** - Comiss√µes de Profissionais

```sql
CREATE TABLE commissions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  professional_id UUID REFERENCES professionals(id) ON DELETE CASCADE,
  order_id UUID REFERENCES orders(id),

  base_amount DECIMAL(10, 2) NOT NULL, -- Valor base para c√°lculo
  commission_rate DECIMAL(5, 2) NOT NULL, -- Percentual
  commission_amount DECIMAL(10, 2) NOT NULL, -- Valor da comiss√£o

  date DATE NOT NULL,
  status VARCHAR(20) DEFAULT 'Pending', -- 'Pending', 'Paid'
  paid_at TIMESTAMPTZ,

  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_commissions_professional_id ON commissions(professional_id);
CREATE INDEX idx_commissions_date ON commissions(date);
CREATE INDEX idx_commissions_status ON commissions(status);
```

#### 15. **lista_da_vez** - Fila de Atendimento

```sql
CREATE TABLE lista_da_vez (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  professional_id UUID REFERENCES professionals(id),

  client_name VARCHAR(255) NOT NULL,
  client_phone VARCHAR(20),
  position INTEGER NOT NULL,
  status VARCHAR(20) DEFAULT 'Waiting', -- 'Waiting', 'In Service', 'Completed', 'Cancelled'

  joined_at TIMESTAMPTZ DEFAULT NOW(),
  called_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,

  notes TEXT
);

CREATE INDEX idx_lista_da_vez_unit_id ON lista_da_vez(unit_id);
CREATE INDEX idx_lista_da_vez_status ON lista_da_vez(status);
CREATE INDEX idx_lista_da_vez_position ON lista_da_vez(position);
```

#### 16. **cash_registers** - Caixas

```sql
CREATE TABLE cash_registers (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  professional_id UUID REFERENCES professionals(id), -- Respons√°vel pelo caixa

  opened_at TIMESTAMPTZ NOT NULL,
  closed_at TIMESTAMPTZ,
  status VARCHAR(20) DEFAULT 'Open', -- 'Open', 'Closed'

  initial_amount DECIMAL(10, 2) NOT NULL,
  final_amount DECIMAL(10, 2),
  expected_amount DECIMAL(10, 2),
  difference_amount DECIMAL(10, 2),

  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_cash_registers_unit_id ON cash_registers(unit_id);
CREATE INDEX idx_cash_registers_status ON cash_registers(status);
CREATE INDEX idx_cash_registers_opened_at ON cash_registers(opened_at);
```

#### 17. **goals** - Metas Financeiras

```sql
CREATE TABLE goals (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,
  category_id UUID REFERENCES categories(id),

  name VARCHAR(255) NOT NULL,
  goal_type VARCHAR(20) NOT NULL, -- 'Revenue', 'Expense', 'Margin', 'Custom'
  target_amount DECIMAL(15, 2) NOT NULL,
  current_amount DECIMAL(15, 2) DEFAULT 0,

  period_type VARCHAR(20) NOT NULL, -- 'Daily', 'Weekly', 'Monthly', 'Yearly'
  start_date DATE NOT NULL,
  end_date DATE NOT NULL,

  status VARCHAR(20) DEFAULT 'Active', -- 'Active', 'Completed', 'Failed', 'Cancelled'

  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_goals_unit_id ON goals(unit_id);
CREATE INDEX idx_goals_status ON goals(status);
CREATE INDEX idx_goals_period ON goals(start_date, end_date);
```

#### 18. **ai_metrics_daily** - M√©tricas Di√°rias (ETL)

Tabela gerada pelo ETL di√°rio com m√©tricas agregadas.

```sql
CREATE TABLE ai_metrics_daily (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,

  metric_date DATE NOT NULL,

  -- Receitas
  gross_revenue DECIMAL(15, 2) DEFAULT 0,
  net_revenue DECIMAL(15, 2) DEFAULT 0,
  revenue_count INTEGER DEFAULT 0,

  -- Despesas
  total_expenses DECIMAL(15, 2) DEFAULT 0,
  fixed_expenses DECIMAL(15, 2) DEFAULT 0,
  variable_expenses DECIMAL(15, 2) DEFAULT 0,
  expense_count INTEGER DEFAULT 0,

  -- Margens
  gross_profit DECIMAL(15, 2) DEFAULT 0,
  gross_margin_percentage DECIMAL(5, 2) DEFAULT 0,
  net_profit DECIMAL(15, 2) DEFAULT 0,
  net_margin_percentage DECIMAL(5, 2) DEFAULT 0,

  -- KPIs
  average_ticket DECIMAL(10, 2) DEFAULT 0,
  customer_count INTEGER DEFAULT 0,
  services_count INTEGER DEFAULT 0,

  -- Metadados
  etl_run_id UUID, -- Refer√™ncia ao etl_runs
  ai_analysis TEXT, -- An√°lise gerada pela IA
  has_anomalies BOOLEAN DEFAULT false,

  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),

  UNIQUE(unit_id, metric_date)
);

CREATE INDEX idx_ai_metrics_unit_id ON ai_metrics_daily(unit_id);
CREATE INDEX idx_ai_metrics_date ON ai_metrics_daily(metric_date);
CREATE INDEX idx_ai_metrics_has_anomalies ON ai_metrics_daily(has_anomalies);
```

#### 19. **alerts** - Alertas do Sistema

```sql
CREATE TABLE alerts (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,

  alert_type VARCHAR(50) NOT NULL, -- 'Anomaly', 'Goal', 'Balance', 'System', 'Cost'
  severity VARCHAR(20) NOT NULL, -- 'Info', 'Warning', 'Critical'
  title VARCHAR(255) NOT NULL,
  message TEXT NOT NULL,

  -- Contexto
  related_entity_type VARCHAR(50), -- 'Revenue', 'Expense', 'Goal', etc.
  related_entity_id UUID,
  metadata JSONB, -- Dados adicionais em JSON

  -- Status
  status VARCHAR(20) DEFAULT 'Pending', -- 'Pending', 'Sent', 'Acknowledged', 'Resolved'
  sent_at TIMESTAMPTZ,
  acknowledged_at TIMESTAMPTZ,
  resolved_at TIMESTAMPTZ,
  resolved_by UUID,

  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_alerts_unit_id ON alerts(unit_id);
CREATE INDEX idx_alerts_type ON alerts(alert_type);
CREATE INDEX idx_alerts_severity ON alerts(severity);
CREATE INDEX idx_alerts_status ON alerts(status);
CREATE INDEX idx_alerts_created_at ON alerts(created_at);
```

#### 20. **kpi_targets** - Metas de KPIs

```sql
CREATE TABLE kpi_targets (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id) ON DELETE CASCADE,

  kpi_name VARCHAR(100) NOT NULL, -- 'gross_revenue', 'net_margin', 'average_ticket', etc.
  target_value DECIMAL(15, 2) NOT NULL,
  period VARCHAR(20) NOT NULL, -- 'Daily', 'Weekly', 'Monthly', 'Yearly'
  start_date DATE NOT NULL,
  end_date DATE,

  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_kpi_targets_unit_id ON kpi_targets(unit_id);
CREATE INDEX idx_kpi_targets_kpi_name ON kpi_targets(kpi_name);
CREATE INDEX idx_kpi_targets_period ON kpi_targets(period);
```

#### 21. **etl_runs** - Rastreamento de Execu√ß√µes ETL (v4.0)

Tabela para idempot√™ncia e tracking de cron jobs.

```sql
CREATE TABLE etl_runs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

  run_type VARCHAR(50) NOT NULL, -- 'ETL_DIARIO', 'RELATORIO_SEMANAL', etc.
  run_date DATE NOT NULL,
  status VARCHAR(20) NOT NULL, -- 'RUNNING', 'SUCCESS', 'FAILED', 'PARTIAL'

  started_at TIMESTAMPTZ DEFAULT NOW(),
  finished_at TIMESTAMPTZ,
  duration_seconds INTEGER,

  units_processed INTEGER DEFAULT 0,
  records_inserted INTEGER DEFAULT 0,
  records_updated INTEGER DEFAULT 0,
  records_failed INTEGER DEFAULT 0,

  error_message TEXT,
  error_stack TEXT,

  trigger_source VARCHAR(50), -- 'cron', 'manual', 'api'
  triggered_by UUID, -- user_id if manual

  metadata JSONB,

  created_at TIMESTAMPTZ DEFAULT NOW(),

  UNIQUE(run_type, run_date, status) -- Permite uma execu√ß√£o SUCCESS por dia
);

CREATE INDEX idx_etl_runs_type ON etl_runs(run_type);
CREATE INDEX idx_etl_runs_date ON etl_runs(run_date);
CREATE INDEX idx_etl_runs_status ON etl_runs(status);
CREATE INDEX idx_etl_runs_started_at ON etl_runs(started_at);
```

#### 22. **openai_cache** - Cache de An√°lises IA (v4.0)

```sql
CREATE TABLE openai_cache (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  cache_key VARCHAR(255) UNIQUE NOT NULL,
  response TEXT NOT NULL,

  tokens_used INTEGER,
  cost_usd DECIMAL(10, 6),
  model VARCHAR(50),

  hit_count INTEGER DEFAULT 0, -- Quantas vezes foi reutilizado

  created_at TIMESTAMPTZ DEFAULT NOW(),
  last_accessed_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_openai_cache_key ON openai_cache(cache_key);
CREATE INDEX idx_openai_cache_created_at ON openai_cache(created_at);
CREATE INDEX idx_openai_cache_last_accessed ON openai_cache(last_accessed_at);

-- Fun√ß√£o de cleanup autom√°tico
CREATE OR REPLACE FUNCTION fn_cleanup_old_cache()
RETURNS void AS $$
BEGIN
  DELETE FROM openai_cache
  WHERE created_at < NOW() - INTERVAL '7 days';
END;
$$ LANGUAGE plpgsql;
```

#### 23. **openai_cost_tracking** - Rastreamento de Custos OpenAI (v4.0)

```sql
CREATE TABLE openai_cost_tracking (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  unit_id UUID REFERENCES units(id),

  date DATE NOT NULL,
  model VARCHAR(50) NOT NULL,

  total_requests INTEGER DEFAULT 0,
  cache_hits INTEGER DEFAULT 0,
  cache_misses INTEGER DEFAULT 0,

  tokens_used INTEGER NOT NULL,
  cost_usd DECIMAL(10, 6) NOT NULL,

  context VARCHAR(100), -- 'etl_diario', 'relatorio_semanal', etc.

  created_at TIMESTAMPTZ DEFAULT NOW(),

  UNIQUE(unit_id, date, model, context)
);

CREATE INDEX idx_cost_tracking_date ON openai_cost_tracking(date);
CREATE INDEX idx_cost_tracking_unit_date ON openai_cost_tracking(unit_id, date);
CREATE INDEX idx_cost_tracking_model ON openai_cost_tracking(model);
```

### Views Materializadas

#### vw_demonstrativo_fluxo - DRE Completo

```sql
CREATE MATERIALIZED VIEW vw_demonstrativo_fluxo AS
SELECT
  unit_id,
  DATE_TRUNC('month', data_competencia) AS month_period,

  -- Receitas
  SUM(CASE WHEN category_type = 'Receita' THEN valor ELSE 0 END) AS total_revenue,

  -- Despesas por tipo
  SUM(CASE WHEN c.name ILIKE '%folha%' THEN valor ELSE 0 END) AS payroll_expenses,
  SUM(CASE WHEN c.name ILIKE '%aluguel%' THEN valor ELSE 0 END) AS rent_expenses,
  SUM(CASE WHEN c.name ILIKE '%energia%' OR c.name ILIKE '%√°gua%' THEN valor ELSE 0 END) AS utility_expenses,
  SUM(CASE WHEN category_type = 'Despesa' THEN valor ELSE 0 END) AS total_expenses,

  -- Resultado
  SUM(CASE WHEN category_type = 'Receita' THEN valor ELSE -valor END) AS net_result

FROM (
  SELECT unit_id, category_id, valor, data_competencia FROM revenues
  UNION ALL
  SELECT unit_id, category_id, valor, data_competencia FROM expenses
) AS transactions
JOIN categories c ON transactions.category_id = c.id
GROUP BY unit_id, DATE_TRUNC('month', data_competencia);

CREATE INDEX idx_vw_dre_unit_period ON vw_demonstrativo_fluxo(unit_id, month_period);
```

### Fun√ß√µes e Stored Procedures

#### fn_calculate_dre - Calcular DRE

```sql
CREATE OR REPLACE FUNCTION fn_calculate_dre(
  p_unit_id UUID,
  p_start_date DATE,
  p_end_date DATE
)
RETURNS TABLE(
  category_name VARCHAR,
  category_type VARCHAR,
  total_amount DECIMAL(15,2)
) AS $$
BEGIN
  RETURN QUERY
  SELECT
    c.name,
    c.category_type,
    SUM(r.valor) AS total
  FROM revenues r
  JOIN categories c ON r.category_id = c.id
  WHERE r.unit_id = p_unit_id
    AND r.data_competencia BETWEEN p_start_date AND p_end_date
  GROUP BY c.name, c.category_type

  UNION ALL

  SELECT
    c.name,
    c.category_type,
    SUM(e.valor) AS total
  FROM expenses e
  JOIN categories c ON e.category_id = c.id
  WHERE e.unit_id = p_unit_id
    AND e.data_competencia BETWEEN p_start_date AND p_end_date
  GROUP BY c.name, c.category_type

  ORDER BY category_type, category_name;
END;
$$ LANGUAGE plpgsql;
```

### Row Level Security (RLS) Policies

Todas as tabelas sens√≠veis t√™m RLS habilitado com pol√≠ticas baseadas em `unit_id`:

```sql
-- Pol√≠tica gen√©rica para tabelas com unit_id
CREATE POLICY "Users can only see data from their unit"
  ON {table_name} FOR ALL
  USING (unit_id::text = auth.jwt() ->> 'unit_id');

-- Para administradores (ver todas as unidades)
CREATE POLICY "Admins can see all data"
  ON {table_name} FOR ALL
  USING (auth.jwt() ->> 'role' = 'admin');
```

---

## üîÄ Fluxogramas de Comunica√ß√£o

### 1. Fluxo de Autentica√ß√£o e Autoriza√ß√£o

```mermaid
sequenceDiagram
    actor User
    participant Browser
    participant VercelCDN as Vercel CDN
    participant ReactApp as React App
    participant SupabaseAuth as Supabase Auth
    participant SupabaseDB as Supabase DB

    User->>Browser: Acessa aplica√ß√£o
    Browser->>VercelCDN: GET /
    VercelCDN->>Browser: index.html + JS bundle
    Browser->>ReactApp: Inicializa app

    alt N√£o autenticado
        ReactApp->>User: Exibe LoginPage
        User->>ReactApp: Submete email/senha
        ReactApp->>SupabaseAuth: signInWithPassword()
        SupabaseAuth-->>ReactApp: JWT token + user data
        ReactApp->>ReactApp: Armazena token (localStorage)
        ReactApp->>SupabaseDB: SELECT user profile
        SupabaseDB-->>ReactApp: User + unit_id + role
        ReactApp->>User: Redireciona para Dashboard
    else J√° autenticado
        ReactApp->>SupabaseAuth: getSession()
        SupabaseAuth-->>ReactApp: Session v√°lida
        ReactApp->>User: Carrega Dashboard
    end

    Note over ReactApp,SupabaseDB: RLS Policies aplicadas<br/>baseadas em unit_id do JWT
```

### 2. Fluxo de Leitura de Dados (Dashboard)

```mermaid
sequenceDiagram
    actor User
    participant DashboardPage
    participant useKPIs as useKPIs Hook
    participant ReactQuery as React Query
    participant revenueRepo as Revenue Repository
    participant SupabaseClient as Supabase Client
    participant SupabaseDB as Supabase DB (RLS)

    User->>DashboardPage: Acessa /dashboard
    DashboardPage->>useKPIs: Chama hook
    useKPIs->>ReactQuery: useQuery('kpis')

    alt Cache v√°lido (< 5min)
        ReactQuery-->>useKPIs: Retorna dados do cache
        useKPIs-->>DashboardPage: KPIs
        DashboardPage->>User: Exibe dashboard
    else Cache inv√°lido / primeira carga
        ReactQuery->>revenueRepo: getKPIs(unitId, dateRange)
        revenueRepo->>SupabaseClient: Query com filtros
        SupabaseClient->>SupabaseDB: SELECT com RLS
        Note over SupabaseDB: RLS filtra por unit_id<br/>do JWT do usu√°rio
        SupabaseDB-->>SupabaseClient: Dados filtrados
        SupabaseClient-->>revenueRepo: Resultado
        revenueRepo->>revenueRepo: Calcula m√©tricas
        revenueRepo-->>ReactQuery: KPIs calculados
        ReactQuery->>ReactQuery: Armazena em cache
        ReactQuery-->>useKPIs: KPIs
        useKPIs-->>DashboardPage: KPIs
        DashboardPage->>User: Exibe dashboard
    end
```

### 3. Fluxo de Cria√ß√£o de Receita

```mermaid
sequenceDiagram
    actor User
    participant RevenuesPage
    participant useRevenues as useRevenues Hook
    participant ReactQuery as React Query
    participant revenueService as Revenue Service
    participant revenueRepo as Revenue Repository
    participant SupabaseClient as Supabase Client
    participant SupabaseDB as Supabase DB
    participant Toast

    User->>RevenuesPage: Preenche formul√°rio
    User->>RevenuesPage: Clica "Salvar"
    RevenuesPage->>RevenuesPage: Valida com Zod

    alt Valida√ß√£o falha
        RevenuesPage->>User: Exibe erros no form
    else Valida√ß√£o OK
        RevenuesPage->>useRevenues: createRevenue(data)
        useRevenues->>ReactQuery: useMutation
        ReactQuery->>revenueService: create(data)
        revenueService->>revenueService: Business logic validations
        revenueService->>revenueRepo: insert(revenue)
        revenueRepo->>SupabaseClient: INSERT INTO revenues
        SupabaseClient->>SupabaseDB: SQL INSERT com unit_id

        alt Sucesso
            SupabaseDB-->>SupabaseClient: Revenue created
            SupabaseClient-->>revenueRepo: New revenue
            revenueRepo-->>revenueService: New revenue
            revenueService-->>ReactQuery: Success
            ReactQuery->>ReactQuery: Invalidate cache 'revenues'
            ReactQuery->>ReactQuery: Refetch queries
            ReactQuery-->>useRevenues: Success
            useRevenues->>Toast: Sucesso!
            Toast->>User: "Receita criada com sucesso"
            RevenuesPage->>User: Atualiza lista
        else Erro
            SupabaseDB-->>SupabaseClient: Error
            SupabaseClient-->>revenueRepo: Error
            revenueRepo-->>revenueService: Error
            revenueService-->>ReactQuery: Error
            ReactQuery-->>useRevenues: Error
            useRevenues->>Toast: Erro!
            Toast->>User: "Erro ao criar receita"
        end
    end
```

### 4. Fluxo de ETL Di√°rio (Cron Job)

```mermaid
flowchart TD
    Start([‚è∞ Vercel Cron<br/>03:00 BRT]) --> Auth{Autorizado?<br/>CRON_SECRET}

    Auth -->|N√£o| Unauthorized[‚ùå 401 Unauthorized]
    Unauthorized --> End1([Fim])

    Auth -->|Sim| Idempotency[üîç Check Idempot√™ncia<br/>ensureIdempotency]

    Idempotency --> Already{J√° executado<br/>hoje?}

    Already -->|Sim| Skip[‚è≠Ô∏è Skip: j√° processado]
    Skip --> Return1[üì§ Return success<br/>skipped: true]
    Return1 --> End2([Fim])

    Already -->|N√£o| CreateRun[üìù Create etl_run<br/>status: RUNNING]

    CreateRun --> FetchUnits[üìä Fetch active units<br/>FROM units WHERE is_active]

    FetchUnits --> HasUnits{Units > 0?}

    HasUnits -->|N√£o| NoUnits[‚ö†Ô∏è Sem unidades ativas]
    NoUnits --> UpdateFailed[‚ùå Update etl_run<br/>status: FAILED]
    UpdateFailed --> End3([Fim])

    HasUnits -->|Sim| ParallelProcess[‚ö° Process in batches<br/>5 units parallel]

    ParallelProcess --> ForEach[üì¶ For each unit]

    ForEach --> ExtractData[üîÑ EXTRACT<br/>Revenues + Expenses]

    ExtractData --> TransformData[üîß TRANSFORM<br/>Calculate metrics<br/>Danfo.js + Decimal.js]

    TransformData --> DetectAnomalies[üîç Detect anomalies<br/>Z-score analysis]

    DetectAnomalies --> CheckCache{Cache exists<br/>for metrics?}

    CheckCache -->|Sim| CacheHit[üíæ Cache HIT<br/>Use cached analysis]
    CacheHit --> SaveMetrics

    CheckCache -->|N√£o| CacheMiss[‚ö†Ô∏è Cache MISS]
    CacheMiss --> Anonymize[üîí Anonymize data<br/>Remove PII]

    Anonymize --> CircuitBreaker{Circuit Breaker<br/>state?}

    CircuitBreaker -->|OPEN| CBOpen[‚ùå Circuit Breaker OPEN<br/>Skip OpenAI]
    CBOpen --> SaveMetrics

    CircuitBreaker -->|CLOSED/HALF_OPEN| CallOpenAI[ü§ñ Call OpenAI<br/>With Retry + Backoff]

    CallOpenAI --> OpenAISuccess{Success?}

    OpenAISuccess -->|N√£o| OpenAIFail[‚ùå OpenAI failed<br/>Update circuit breaker]
    OpenAIFail --> SaveMetrics

    OpenAISuccess -->|Sim| TrackCost[üí∞ Track cost<br/>openai_cost_tracking]
    TrackCost --> SaveCache[üíæ Save to cache<br/>openai_cache]
    SaveCache --> CheckThreshold{Cost > 80%<br/>threshold?}

    CheckThreshold -->|Sim| AlertCost[üö® Alert: High cost]
    AlertCost --> SaveMetrics

    CheckThreshold -->|N√£o| SaveMetrics[üíæ LOAD<br/>Save to ai_metrics_daily]

    SaveMetrics --> CheckKPIs[üìä Check KPI targets]

    CheckKPIs --> KPIMissed{KPI<br/>missed?}

    KPIMissed -->|Sim| CreateAlert[üö® Create alert<br/>INSERT INTO alerts]
    CreateAlert --> NextUnit

    KPIMissed -->|N√£o| NextUnit{More units?}

    NextUnit -->|Sim| ForEach

    NextUnit -->|N√£o| Aggregate[üìä Aggregate results]

    Aggregate --> UpdateSuccess[‚úÖ Update etl_run<br/>status: SUCCESS<br/>units_processed, records_inserted]

    UpdateSuccess --> SendTelegram[üì± Send summary<br/>Telegram notification]

    SendTelegram --> TelegramSuccess{Success?}

    TelegramSuccess -->|Sim| LogSuccess[üìù Log: ETL completed]
    TelegramSuccess -->|N√£o| LogWarning[‚ö†Ô∏è Log: ETL OK but Telegram failed]

    LogSuccess --> End4([‚úÖ Fim Sucesso])
    LogWarning --> End5([‚ö†Ô∏è Fim com Aviso])

    style Start fill:#4ecdc4
    style End4 fill:#95e1d3
    style End5 fill:#f38181
    style CallOpenAI fill:#10a37f
    style SaveCache fill:#61dafb
    style ParallelProcess fill:#ff6b6b
```

### 5. Fluxo de Health Check (Cron Job)

```mermaid
flowchart TD
    Start([‚è∞ Vercel Cron<br/>Every 5 min]) --> Auth{Autorizado?}

    Auth -->|N√£o| Unauthorized[‚ùå 401 Unauthorized]
    Unauthorized --> End1([Fim])

    Auth -->|Sim| Parallel[üîÄ Run checks in parallel]

    Parallel --> CheckSupabase[üóÑÔ∏è Check Supabase]
    Parallel --> CheckOpenAI[ü§ñ Check OpenAI]
    Parallel --> CheckTelegram[üì± Check Telegram]
    Parallel --> CheckLastCron[‚è∞ Check Last Cron]
    Parallel --> CheckStorage[üíæ Check Storage]

    CheckSupabase --> SupabaseTest[Query: SELECT id<br/>FROM units LIMIT 1]
    SupabaseTest --> SupabaseResult{Success?}
    SupabaseResult -->|Sim| SupabaseOK[‚úÖ Supabase: healthy<br/>latency: Xms]
    SupabaseResult -->|N√£o| SupabaseFail[‚ùå Supabase: unhealthy]

    CheckOpenAI --> OpenAICost[Get monthly cost<br/>FROM openai_cost_tracking]
    OpenAICost --> OpenAIThreshold{Cost <<br/>threshold?}
    OpenAIThreshold -->|Sim| OpenAIOK[‚úÖ OpenAI: healthy<br/>cost: $X]
    OpenAIThreshold -->|N√£o| OpenAIFail[‚ùå OpenAI: unhealthy<br/>cost exceeded]

    CheckTelegram --> TelegramTest[Send test message<br/>getMe API]
    TelegramTest --> TelegramResult{Success?}
    TelegramResult -->|Sim| TelegramOK[‚úÖ Telegram: healthy]
    TelegramResult -->|N√£o| TelegramFail[‚ùå Telegram: unhealthy]

    CheckLastCron --> LastCronQuery[Query: last ETL run<br/>FROM etl_runs]
    LastCronQuery --> LastCronTime{< 25h ago<br/>AND SUCCESS?}
    LastCronTime -->|Sim| CronOK[‚úÖ Last Cron: healthy]
    LastCronTime -->|N√£o| CronFail[‚ùå Last Cron: unhealthy]

    CheckStorage --> StorageQuery[Get usage<br/>Supabase Storage API]
    StorageQuery --> StorageLimit{< 80%?}
    StorageLimit -->|Sim| StorageOK[‚úÖ Storage: healthy]
    StorageLimit -->|N√£o| StorageFail[‚ùå Storage: unhealthy]

    SupabaseOK --> Aggregate
    SupabaseFail --> Aggregate
    OpenAIOK --> Aggregate
    OpenAIFail --> Aggregate
    TelegramOK --> Aggregate
    TelegramFail --> Aggregate
    CronOK --> Aggregate
    CronFail --> Aggregate
    StorageOK --> Aggregate
    StorageFail --> Aggregate

    Aggregate[üìä Aggregate results] --> AllHealthy{All checks<br/>healthy?}

    AllHealthy -->|Sim| Healthy[‚úÖ System: healthy]
    Healthy --> LogHealthy[üìù Log success]
    LogHealthy --> ReturnHealthy[üì§ Return 200<br/>status: healthy]
    ReturnHealthy --> End2([Fim])

    AllHealthy -->|N√£o| Unhealthy[‚ùå System: degraded]
    Unhealthy --> FormatAlert[üìã Format alert message]
    FormatAlert --> SendAlert[üì± Send Telegram alert<br/>‚ö†Ô∏è Health Check Failed]
    SendAlert --> LogUnhealthy[üìù Log warning]
    LogUnhealthy --> ReturnUnhealthy[üì§ Return 200<br/>status: degraded]
    ReturnUnhealthy --> End3([Fim])

    style Start fill:#4ecdc4
    style Healthy fill:#95e1d3
    style Unhealthy fill:#f38181
    style Parallel fill:#feca57
```

### 6. Fluxo de Reconcilia√ß√£o Banc√°ria

```mermaid
sequenceDiagram
    actor User
    participant ReconciliationPage
    participant FileUpload
    participant OFXParser
    participant ReconciliationService
    participant MatchingEngine
    participant SupabaseDB
    participant Toast

    User->>ReconciliationPage: Acessa p√°gina
    User->>FileUpload: Seleciona arquivo OFX/Excel
    User->>FileUpload: Upload

    FileUpload->>OFXParser: Parse file
    OFXParser->>OFXParser: Validate format

    alt Formato inv√°lido
        OFXParser-->>User: Erro: formato n√£o suportado
    else Formato v√°lido
        OFXParser->>OFXParser: Extract transactions
        OFXParser->>OFXParser: Generate source_hash
        OFXParser-->>ReconciliationService: List<Transaction>

        ReconciliationService->>SupabaseDB: Check for duplicates<br/>(by source_hash)
        SupabaseDB-->>ReconciliationService: Duplicates list

        ReconciliationService->>ReconciliationService: Filter new transactions

        ReconciliationService->>SupabaseDB: Bulk INSERT<br/>bank_statements
        SupabaseDB-->>ReconciliationService: Inserted statements

        ReconciliationService->>MatchingEngine: Auto-match transactions

        loop For each bank statement
            MatchingEngine->>SupabaseDB: Query revenues/expenses<br/>by amount + date
            SupabaseDB-->>MatchingEngine: Candidates

            MatchingEngine->>MatchingEngine: Calculate similarity<br/>(Levenshtein distance)
            MatchingEngine->>MatchingEngine: Assign confidence score

            alt Confidence > 90%
                MatchingEngine->>SupabaseDB: INSERT reconciliation<br/>status: Matched
            else Confidence 70-90%
                MatchingEngine->>SupabaseDB: INSERT reconciliation<br/>status: Pending
            else Confidence < 70%
                MatchingEngine->>MatchingEngine: Skip (no match)
            end
        end

        MatchingEngine-->>ReconciliationService: Matching results

        ReconciliationService-->>ReconciliationPage: Summary stats
        ReconciliationPage->>Toast: Sucesso!
        Toast->>User: "X transa√ß√µes importadas<br/>Y matches autom√°ticos"

        ReconciliationPage->>User: Exibe lista de matches<br/>pendentes para revis√£o

        User->>ReconciliationPage: Revisa match sugerido
        User->>ReconciliationPage: Confirma/Rejeita

        ReconciliationPage->>ReconciliationService: updateReconciliation(id, status)
        ReconciliationService->>SupabaseDB: UPDATE reconciliations<br/>status: Confirmed/Rejected

        alt Status: Confirmed
            ReconciliationService->>SupabaseDB: UPDATE revenue/expense<br/>reconciled: true
            ReconciliationService->>SupabaseDB: UPDATE bank_account<br/>current_balance
        end

        SupabaseDB-->>ReconciliationService: Updated
        ReconciliationService-->>User: Success
    end
```

### 7. Fluxo de Cache OpenAI

```mermaid
flowchart TD
    Start([ETL needs AI analysis]) --> GenerateKey[üîë Generate cache key<br/>hash(unitId + metrics)]

    GenerateKey --> CheckCache{Cache exists?<br/>getCachedAnalysis}

    CheckCache -->|Sim| CheckTTL{Age < 24h?}

    CheckTTL -->|Sim| CacheHit[üíæ Cache HIT<br/>Return cached response]
    CacheHit --> IncrementHit[üìä Increment hit_count]
    IncrementHit --> UpdateAccess[üïê Update last_accessed_at]
    UpdateAccess --> LogHit[üìù Log: Cache hit saved $X]
    LogHit --> Return1[üì§ Return analysis]
    Return1 --> End1([Fim])

    CheckTTL -->|N√£o| Expired[‚è∞ Cache EXPIRED]
    Expired --> DeleteExpired[üóëÔ∏è DELETE from cache]
    DeleteExpired --> CacheMiss

    CheckCache -->|N√£o| CacheMiss[‚ùå Cache MISS]

    CacheMiss --> Anonymize[üîí Anonymize data<br/>Remove PII]
    Anonymize --> CallOpenAI[ü§ñ Call OpenAI API<br/>with retry + circuit breaker]

    CallOpenAI --> OpenAIResult{Success?}

    OpenAIResult -->|N√£o| OpenAIFail[‚ùå OpenAI failed]
    OpenAIFail --> FallbackAnalysis[üìù Fallback: basic analysis<br/>without AI]
    FallbackAnalysis --> Return2[üì§ Return fallback]
    Return2 --> End2([Fim])

    OpenAIResult -->|Sim| ParseResponse[üìÑ Parse OpenAI response]

    ParseResponse --> SaveCache[üíæ Save to cache<br/>setCachedAnalysis]

    SaveCache --> TrackCost[üí∞ Track cost & tokens<br/>openai_cost_tracking]

    TrackCost --> CheckCostThreshold{Monthly cost<br/>> threshold?}

    CheckCostThreshold -->|Sim| AlertHighCost[üö® Alert: High OpenAI cost<br/>Send Telegram notification]
    AlertHighCost --> LogMiss

    CheckCostThreshold -->|N√£o| LogMiss[üìù Log: Cache miss cost $X]

    LogMiss --> Return3[üì§ Return analysis]
    Return3 --> End3([Fim])

    style CacheHit fill:#95e1d3
    style CacheMiss fill:#f38181
    style SaveCache fill:#61dafb
    style CallOpenAI fill:#10a37f
```

---

## üöÄ Guia de Deploy

### Deploy Inicial

1. **Setup do Projeto:**
```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/barber-analytics-pro.git
cd barber-analytics-pro

# Instalar depend√™ncias
pnpm install

# Configurar vari√°veis de ambiente
cp .env.example .env.local
# Editar .env.local com valores reais
```

2. **Configurar Supabase:**
```bash
# Instalar Supabase CLI
npm install -g supabase

# Login no Supabase
supabase login

# Link com projeto existente (ou criar novo)
supabase link --project-ref seu-project-ref

# Aplicar migrations
supabase db push

# Validar RLS
supabase db diff
```

3. **Configurar Vercel:**
```bash
# Instalar Vercel CLI
npm install -g vercel

# Login
vercel login

# Link com projeto (ou criar novo)
vercel link

# Configurar env vars
vercel env add VITE_SUPABASE_URL
vercel env add VITE_SUPABASE_ANON_KEY
vercel env add SUPABASE_SERVICE_ROLE_KEY
vercel env add OPENAI_API_KEY
vercel env add TELEGRAM_BOT_TOKEN
vercel env add TELEGRAM_CHAT_ID
vercel env add CRON_SECRET
# ... adicionar todas as outras vari√°veis

# Deploy de preview
vercel

# Deploy de produ√ß√£o
vercel --prod
```

4. **Configurar Cron Jobs:**

Os cron jobs s√£o configurados automaticamente via `vercel.json`. Verificar se foram criados:

```bash
vercel crons ls
```

5. **Validar Deploy:**
```bash
# Testar health check
curl https://seu-dominio.vercel.app/api/health

# Testar cron manual (requer CRON_SECRET)
curl -X GET https://seu-dominio.vercel.app/api/cron/etl-diario \
  -H "Authorization: Bearer $CRON_SECRET"
```

### Deploy Cont√≠nuo (CI/CD)

O projeto usa GitHub Actions para CI/CD. Configurar secrets no GitHub:

1. Ir em Settings ‚Üí Secrets and variables ‚Üí Actions
2. Adicionar secrets:
   - `VERCEL_TOKEN`
   - `VERCEL_ORG_ID`
   - `VERCEL_PROJECT_ID`
   - Todas as env vars necess√°rias

Workflow `.github/workflows/deploy.yml` ir√°:
- Lint e test em PRs
- Deploy preview autom√°tico em branches
- Deploy produ√ß√£o autom√°tico em merge para `main`

---

## üîß Troubleshooting

### Problemas Comuns

#### 1. Cron Job N√£o Executa

**Sintomas:**
- ETL di√°rio n√£o roda √†s 03:00
- Nenhum log no Vercel
- Nenhuma notifica√ß√£o Telegram

**Solu√ß√µes:**
1. Verificar se cron est√° configurado no `vercel.json`
2. Verificar timezone correto (America/Sao_Paulo)
3. Testar execu√ß√£o manual:
```bash
curl -X GET https://seu-dominio.vercel.app/api/cron/etl-diario \
  -H "Authorization: Bearer $CRON_SECRET"
```
4. Verificar logs no Vercel Dashboard

#### 2. OpenAI API Retorna Erro 429 (Rate Limit)

**Sintomas:**
- Circuit breaker abre
- Logs mostram "Rate limit exceeded"
- ETL falha na gera√ß√£o de an√°lise

**Solu√ß√µes:**
1. Verificar quota na dashboard OpenAI
2. Aumentar delay entre requests (ajustar retry config)
3. Reduzir batch size de processamento paralelo
4. Implementar rate limiting no lado do cliente

#### 3. Cache N√£o Funciona

**Sintomas:**
- Todos os requests v√£o para OpenAI (cache miss 100%)
- Custos n√£o reduzem

**Solu√ß√µes:**
1. Verificar se tabela `openai_cache` existe
2. Verificar se `generateCacheKey` est√° gerando keys consistentes
3. Verificar TTL do cache (padr√£o 24h)
4. Limpar cache antigo manualmente:
```sql
DELETE FROM openai_cache WHERE created_at < NOW() - INTERVAL '7 days';
```

#### 4. RLS Bloqueia Queries Leg√≠timas

**Sintomas:**
- Queries retornam vazio mesmo com dados no banco
- Erro "insufficient permissions"

**Solu√ß√µes:**
1. Verificar se JWT cont√©m `unit_id` correto
2. Verificar policies RLS:
```sql
SELECT * FROM pg_policies WHERE tablename = 'revenues';
```
3. Testar query com service role (para debug apenas):
```javascript
const { data } = await supabaseAdmin
  .from('revenues')
  .select('*');
```

#### 5. Frontend N√£o Carrega (404)

**Sintomas:**
- Rotas retornam 404
- SPA n√£o funciona

**Solu√ß√µes:**
1. Verificar rewrite no `vercel.json`:
```json
{
  "rewrites": [
    {
      "source": "/((?!api).*)",
      "destination": "/index.html"
    }
  ]
}
```
2. Verificar build output:
```bash
ls -la dist/
```
3. Verificar se `dist/index.html` existe

#### 6. Build Falha no Vercel

**Sintomas:**
- Deploy falha com erro de build
- Logs mostram erro no Vite

**Solu√ß√µes:**
1. Verificar vers√£o do Node (deve ser >=20.19.0)
2. Verificar `package.json` tem scripts corretos:
```json
{
  "scripts": {
    "build": "vite build"
  }
}
```
3. Rodar build localmente para debug:
```bash
pnpm build
```

---

## üìä M√©tricas de Sucesso

### KPIs de Infraestrutura

| M√©trica | Target | Como Medir |
|---------|--------|------------|
| ETL Success Rate | > 98% | `etl_runs` table |
| ETL Duration | < 5min | `etl_runs.duration_seconds` |
| API Response Time (p95) | < 500ms | Vercel Analytics |
| Cache Hit Rate | > 60% | `openai_cache.hit_count` |
| OpenAI Cost | < $80/m√™s | `openai_cost_tracking` |
| Health Check Uptime | > 99.9% | Health check logs |
| Frontend Build Time | < 2min | Vercel build logs |
| Bundle Size (gzipped) | < 2MB | `dist/` folder |

### Checklist de Go-Live

- [ ] Todas as env vars configuradas
- [ ] Migrations aplicadas
- [ ] RLS policies validadas
- [ ] Cron jobs testados
- [ ] Health checks funcionando
- [ ] Cache funcionando
- [ ] Circuit breaker testado
- [ ] Alertas Telegram configurados
- [ ] Backup strategy definida
- [ ] Monitoring dashboards criados
- [ ] Documenta√ß√£o atualizada
- [ ] Equipe treinada

---

## üìù Notas Finais

Este documento deve ser atualizado conforme novas features s√£o implementadas. Manter sincronizado com `INFRASTRUCTURE_v4.0.md` (agora v5.0).

Para suporte, consultar:
- README.md principal
- Documenta√ß√£o t√©cnica em `/docs`
- Issues no GitHub
- Logs no Vercel Dashboard

**Vers√£o:** 5.0
**√öltima atualiza√ß√£o:** 11 de novembro de 2025
**Pr√≥xima revis√£o:** Trimestral
